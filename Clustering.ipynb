{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59420117",
   "metadata": {},
   "source": [
    "# Custering of a Quantum Convolutional Processed Dataset\n",
    "\n",
    "This notebook contains all the rutines to apply clustering methods to a quantum convolution processed dataset. The script will:\n",
    "\n",
    "1. Load the dataset and print some statistics\n",
    "2. Apply a clustering method\n",
    "3. Showt the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c30cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.datahandler import datahandler\n",
    "from data.datareader import datareader\n",
    "from layers.QConv2D import QConv2D\n",
    "from utils import test_loader\n",
    "from utils.plotter import *\n",
    "\n",
    "from sklearn.cluster import Birch, KMeans, SpectralClustering, DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from numpy import unique\n",
    "from numpy import where\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "\n",
    "SEED = 10\n",
    "from numpy import random\n",
    "random.seed(SEED)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e03f02",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "Instructions\n",
    "1. the dataset should be placed in the working directory, specifically in the **datasets** folder.\n",
    "2. the dataset should be already divided into classes, one-subfolder for earch classes. The folder/class name will be used to encode the label\n",
    "\n",
    "```\n",
    "QuantumCNN\n",
    "│   README.md\n",
    "│   requirements.txt    \n",
    "│\n",
    "└───circuits\n",
    "└───...\n",
    "└───datasets\n",
    "    └───EuroSAT\n",
    "        └───Highway\n",
    "                highway1.jpg\n",
    "                highway2.jpg                \n",
    "        └─── ....\n",
    "        └───Lake\n",
    "                lake1.jpg\n",
    "                lake2.jpg                \n",
    "\n",
    "```\n",
    "\n",
    "Given *the dataset_name*, that must be the same of the folder, the **datahandler** will take care of loading the paths of the images and collected them into a class dictionary. After a report of the dataset will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999ab4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'EuroSAT_processed_QCNN_1'\n",
    "root = os.path.join('datasets', dataset_name)\n",
    "dhandler = datahandler(root)\n",
    "dhandler.print_report(name=dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f67f0c1",
   "metadata": {},
   "source": [
    "The **unpack** function trasforms the dataset from a dictionary to an array. It assigns also the label to each image and returns a dictionary mapping the labels with the class name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334792f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels_mapper, x, y = dhandler.unpack(dhandler.paths)\n",
    "\n",
    "print('Labels')\n",
    "for key in labels_mapper: print('{:<30s}{}'.format(key,labels_mapper[key]))\n",
    "\n",
    "print('\\nDataset Size')\n",
    "print('{:<30s}{}'.format('Images', len(x)))\n",
    "\n",
    "print('\\nTraining Dataset samples')\n",
    "print('{:<30s}{:<80s}{:<10s}{}'.format('X Train', x[0], 'Size', np.load(x[0]).shape))\n",
    "print('{:<30s}{}'.format('X Train', y[0]))\n",
    "classes = list(labels_mapper.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f65cc63",
   "metadata": {},
   "source": [
    "Test the keras-like data loader. In this specific case the *datareader.generatorv2* is tested. It contains all the rutines to load images batch by batch (1 in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c00a4b-c545-412a-81a4-2d28fb603fe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reshape(x):\n",
    "    for i in range(x.shape[-1]):\n",
    "        if i == 0:\n",
    "            vals = x[:,:,i].flatten()\n",
    "        else:\n",
    "            vals = np.concatenate((vals, x[:,:,i].flatten()))\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415a59ef-a372-47af-9d6f-f5a450934bfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader  = iter(datareader.generatorv2((x, y), (3,3,12)))\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for _ in tqdm(range(len(x))):    \n",
    "    it = next(loader)\n",
    "    lbl = np.argmax(it[1])\n",
    "    #if lbl == 0 or lbl == 1:\n",
    "    X.append(reshape(it[0]))\n",
    "    Y.append(lbl)\n",
    "\n",
    "Y = np.array(Y)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dcc907-f335-424e-821f-6681add104b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=10, random_state=SEED)#, svd_solver='full')\n",
    "X_std = pca.fit_transform(X_std)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f621f3-08c2-4694-9d0b-aec4a17e778c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_CLUSTERS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68670828-386b-4ff6-9ebf-6d5258ce191a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "errors, threshold, branching_factor = [], [], []\n",
    "best_err, best_th, best_br = 1000, 0, 0\n",
    "\n",
    "for t in np.flip(np.arange(0.01, 1.01, 0.01)):\n",
    "    for b in range(2, 100, 1):\n",
    "        km = Birch(threshold=t, #0.86, #1, \n",
    "                    branching_factor=b, #9,#5,\n",
    "                    n_clusters=N_CLUSTERS,\n",
    "                    compute_labels=True,\n",
    "                    copy=True)\n",
    "\n",
    "        yhat = km.fit_predict(X_std)\n",
    "        yhatct = -np.sort(-np.bincount(yhat))\n",
    "        yct = -np.sort(-np.bincount(Y))\n",
    "        err = np.sum(np.abs(yct - yhatct))/ np.sum(yct)   \n",
    "        \n",
    "        if err <= best_err:\n",
    "            best_err=err\n",
    "            best_th=t\n",
    "            best_br=b\n",
    "        \n",
    "        print('\\r Current Error {:.5f} - Threshold {:.2f} - Branching Factor {} -- Best Error {:.5f} - Threshold {:.2f} - Branching Factor {} '.format(err,t,b,best_err, best_th, best_br), end='\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t')\n",
    "        errors.append(err)\n",
    "        threshold.append(t)\n",
    "        branching_factor.append(b)\n",
    "\n",
    "M = np.argmin(errors)\n",
    "print('Min {} - Error - {:.5f} - Threshold {:.2f} - Branching Factor {}'.format(M, errors[M], threshold[M], branching_factor[M]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0fb0aa-cc54-4f8d-8bee-959d5f2d95fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "M = np.argmin(errors)\n",
    "print('Min {} - Error - {:.5f} - Threshold {:.2f} - Branching Factor {}'.format(M, errors[M], threshold[M], branching_factor[M]))\n",
    "km = Birch(threshold=threshold[M], #0.86, #1, \n",
    "                    branching_factor=branching_factor[M], #9,#5,\n",
    "                    n_clusters=N_CLUSTERS,\n",
    "                    compute_labels=True,\n",
    "                    copy=True)\n",
    "\n",
    "yhat = km.fit_predict(X_std)\n",
    "yhatct = -np.sort(-np.bincount(yhat))\n",
    "yct = -np.sort(-np.bincount(Y))\n",
    "err = np.sum(np.abs(yct - yhatct))/ np.sum(yct)\n",
    "print('Recomputed Error {:.5f}'.format(err))\n",
    "\n",
    "fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (25, 5), gridspec_kw={'width_ratios': [3, 2]})\n",
    "# retrieve unique clusters\n",
    "clusters = unique(yhat)\n",
    "# create scatter plot for samples from each cluster\n",
    "for i, cluster in enumerate(clusters):\n",
    "    # get row indexes for samples with this cluster\n",
    "    row_ix = where(yhat == cluster)\n",
    "    # create scatter of these samples\n",
    "    ax[0].scatter(X_std[row_ix, 0], X_std[row_ix, 1], marker='+', label=classes[i])\n",
    "    ax[0].set_xlabel('PCA Component 1')\n",
    "    ax[0].set_ylabel('PCA Component 2')\n",
    "\n",
    "ax[0].legend(loc='lower right')\n",
    "\n",
    "xx = np.arange(N_CLUSTERS) - 0.2\n",
    "ax[1].bar(xx,  yhatct, width=0.3, align='center', label='Birch', color='purple')\n",
    "\n",
    "for i, val in enumerate(yhatct):\n",
    "    ax[1].text(i-0.5, 20+val, val, fontsize=8, rotation=30)\n",
    "\n",
    "xx = np.arange(N_CLUSTERS) + 0.2\n",
    "ax[1].bar(xx,  yct, width=0.3, align='center', label='Ground Truth', color='brown')\n",
    "for i, val in enumerate(yct):\n",
    "    ax[1].text(i, 20+val, val, fontsize=8, rotation=30)\n",
    "\n",
    "ax[1].set(xticks=range(N_CLUSTERS), xlim=[-1, N_CLUSTERS])    \n",
    "ax[1].set_xticklabels(classes, rotation=45, ha='right')\n",
    "#ax[1].set_xlabel('Cluster')\n",
    "ax[1].set_ylabel('#images')\n",
    "\n",
    "mm = np.bincount(yhat).max()\n",
    "ax[1].set_ylim([None,mm+(20*mm)/100])\n",
    "ax[1].legend(loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57e58d1-94bd-4812-a93c-29b725e7073d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
