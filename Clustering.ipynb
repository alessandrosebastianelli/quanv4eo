{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59420117",
   "metadata": {},
   "source": [
    "# Custering of a Quantum Convolutional Processed Dataset\n",
    "\n",
    "This notebook contains all the rutines to apply clustering methods to a quantum convolution processed dataset. The script will:\n",
    "\n",
    "1. Load the dataset and print some statistics\n",
    "2. Apply a clustering method\n",
    "3. Showt the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8c30cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asebastianelli/miniforge3/envs/jax/lib/python3.10/site-packages/jax/_src/lib/__init__.py:32: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n",
      "  warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\n",
      "/Users/asebastianelli/miniforge3/envs/jax/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "from data.datahandler import datahandler\n",
    "from data.datareader import datareader\n",
    "from layers.QConv2D import QConv2D\n",
    "from utils import test_loader\n",
    "from utils.plotter import *\n",
    "\n",
    "from sklearn.cluster import Birch, KMeans, SpectralClustering, DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from numpy import unique\n",
    "from numpy import where\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "\n",
    "SEED = 10\n",
    "from numpy import random\n",
    "random.seed(SEED)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e03f02",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "Instructions\n",
    "1. the dataset should be placed in the working directory, specifically in the **datasets** folder.\n",
    "2. the dataset should be already divided into classes, one-subfolder for earch classes. The folder/class name will be used to encode the label\n",
    "\n",
    "```\n",
    "QuantumCNN\n",
    "│   README.md\n",
    "│   requirements.txt    \n",
    "│\n",
    "└───circuits\n",
    "└───...\n",
    "└───datasets\n",
    "    └───EuroSAT\n",
    "        └───Highway\n",
    "                highway1.jpg\n",
    "                highway2.jpg                \n",
    "        └─── ....\n",
    "        └───Lake\n",
    "                lake1.jpg\n",
    "                lake2.jpg                \n",
    "\n",
    "```\n",
    "\n",
    "Given *the dataset_name*, that must be the same of the folder, the **datahandler** will take care of loading the paths of the images and collected them into a class dictionary. After a report of the dataset will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "999ab4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "\n",
      "Dataset EuroSAT_processed_QCNN_1\n",
      "\n",
      "Class 0 - Forest                    - #images: 3000\n",
      "Class 1 - River                     - #images: 2500\n",
      "Class 2 - Highway                   - #images: 2500\n",
      "Class 3 - AnnualCrop                - #images: 3000\n",
      "Class 4 - SeaLake                   - #images: 3000\n",
      "Class 5 - HerbaceousVegetation      - #images: 3000\n",
      "Class 6 - Industrial                - #images: 2500\n",
      "Class 7 - Residential               - #images: 3000\n",
      "Class 8 - PermanentCrop             - #images: 2500\n",
      "Class 9 - Pasture                   - #images: 2000\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'EuroSAT_processed_QCNN_1'\n",
    "root = os.path.join('datasets', dataset_name)\n",
    "dhandler = datahandler(root)\n",
    "dhandler.print_report(name=dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f67f0c1",
   "metadata": {},
   "source": [
    "The **unpack** function trasforms the dataset from a dictionary to an array. It assigns also the label to each image and returns a dictionary mapping the labels with the class name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "334792f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels\n",
      "Forest                        [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "River                         [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Highway                       [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "AnnualCrop                    [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "SeaLake                       [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "HerbaceousVegetation          [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "Industrial                    [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "Residential                   [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "PermanentCrop                 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "Pasture                       [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Dataset Size\n",
      "Images                        27000\n",
      "\n",
      "Training Dataset samples\n",
      "X Train                       datasets/EuroSAT_processed_QCNN_1/Forest/Forest_576.npy                         Size      (3, 3, 12)\n",
      "X Train                       [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "labels_mapper, x, y = dhandler.unpack(dhandler.paths)\n",
    "\n",
    "print('Labels')\n",
    "for key in labels_mapper: print('{:<30s}{}'.format(key,labels_mapper[key]))\n",
    "\n",
    "print('\\nDataset Size')\n",
    "print('{:<30s}{}'.format('Images', len(x)))\n",
    "\n",
    "print('\\nTraining Dataset samples')\n",
    "print('{:<30s}{:<80s}{:<10s}{}'.format('X Train', x[0], 'Size', np.load(x[0]).shape))\n",
    "print('{:<30s}{}'.format('X Train', y[0]))\n",
    "classes = list(labels_mapper.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f65cc63",
   "metadata": {},
   "source": [
    "Test the keras-like data loader. In this specific case the *datareader.generatorv2* is tested. It contains all the rutines to load images batch by batch (1 in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "30c00a4b-c545-412a-81a4-2d28fb603fe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reshape(x):\n",
    "    for i in range(x.shape[-1]):\n",
    "        if i == 0:\n",
    "            vals = x[:,:,i].flatten()\n",
    "        else:\n",
    "            vals = np.concatenate((vals, x[:,:,i].flatten()))\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "415a59ef-a372-47af-9d6f-f5a450934bfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5da76595db4aa4a9b42c05a00ed26b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loader  = iter(datareader.generatorv2((x, y), (3,3,12)))\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for _ in tqdm(range(len(x))):    \n",
    "    it = next(loader)\n",
    "    lbl = np.argmax(it[1])\n",
    "    #if lbl == 0 or lbl == 1:\n",
    "    X.append(reshape(it[0]))\n",
    "    Y.append(lbl)\n",
    "\n",
    "Y = np.array(Y)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "27dcc907-f335-424e-821f-6681add104b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29618874 0.067007   0.06227834 0.06162145 0.03699009 0.034106\n",
      " 0.03044137 0.02813536 0.02328396 0.02267668]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=10, random_state=SEED)#, svd_solver='full')\n",
    "X_std = pca.fit_transform(X_std)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34f621f3-08c2-4694-9d0b-aec4a17e778c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_CLUSTERS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68670828-386b-4ff6-9ebf-6d5258ce191a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Current Error 0.30274074074074075 - Threshold 0.9500000000000001 - Branching Factor 86 -- Best Errorr 0.17429629629629628 - Threshold 0.9500000000000001 - Branching Factor 25 \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t"
     ]
    }
   ],
   "source": [
    "errors, threshold, branching_factor = [], [], []\n",
    "best_err, best_th, best_br = 1000, 0, 0\n",
    "\n",
    "for t in np.flip(np.arange(0.01, 1.01, 0.01)):\n",
    "    for b in range(2, 100, 1):\n",
    "        km = Birch(threshold=t, #0.86, #1, \n",
    "                    branching_factor=b, #9,#5,\n",
    "                    n_clusters=N_CLUSTERS,\n",
    "                    compute_labels=True,\n",
    "                    copy=True)\n",
    "\n",
    "        yhat = km.fit_predict(X_std)\n",
    "        yhatct = -np.sort(-np.bincount(yhat))\n",
    "        yct = -np.sort(-np.bincount(Y))\n",
    "        err = np.sum(np.abs(yct - yhatct))/ np.sum(yct)   \n",
    "        \n",
    "        if err <= best_err:\n",
    "            best_err=err\n",
    "            best_th=t\n",
    "            best_br=b\n",
    "        \n",
    "        print('\\r Current Error {} - Threshold {} - Branching Factor {} -- Best Errorr {} - Threshold {} - Branching Factor {} '.format(err,t,b,best_err, best_th, best_br), end='\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t')\n",
    "        errors.append(err)\n",
    "        threshold.append(t)\n",
    "        branching_factor.append(b)\n",
    "\n",
    "M = np.argmin(errors)\n",
    "print('Min {} - Error - {} - Threshold {} - Branching Factor {}'.format(M, errors[M], threshold[M], branching_factor[M]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0fb0aa-cc54-4f8d-8bee-959d5f2d95fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "M = np.argmin(errors)\n",
    "print('Min {} - Error - {} - Threshold {} - Branching Factor {}'.format(M, errors[M], threshold[M], branching_factor[M]))\n",
    "km = Birch(threshold=threshold[M], #0.86, #1, \n",
    "                    branching_factor=branching_factor[M], #9,#5,\n",
    "                    n_clusters=N_CLUSTERS,\n",
    "                    compute_labels=True,\n",
    "                    copy=True)\n",
    "\n",
    "yhat = km.fit_predict(X_std)\n",
    "yhatct = -np.sort(-np.bincount(yhat))\n",
    "yct = -np.sort(-np.bincount(Y))\n",
    "err = np.sum(np.abs(yct - yhatct))/ np.sum(yct)\n",
    "print('Recomputed Error {}'.format(err))\n",
    "\n",
    "fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (20, 5), gridspec_kw={'width_ratios': [3, 1]})\n",
    "# retrieve unique clusters\n",
    "clusters = unique(yhat)\n",
    "# create scatter plot for samples from each cluster\n",
    "for i, cluster in enumerate(clusters):\n",
    "    # get row indexes for samples with this cluster\n",
    "    row_ix = where(yhat == cluster)\n",
    "    # create scatter of these samples\n",
    "    ax[0].scatter(X_std[row_ix, 0], X_std[row_ix, 1], marker='+')\n",
    "    ax[0].set_xlabel('PCA Component 1')\n",
    "    ax[0].set_ylabel('PCA Component 2')\n",
    "\n",
    "xx = np.arange(N_CLUSTERS) - 0.2\n",
    "ax[1].bar(xx,  yhatct, width=0.3, align='center', label='Birch', color='purple')\n",
    "\n",
    "for i, val in enumerate(yhatct):\n",
    "    ax[1].text(i-0.5, 20+val, val)\n",
    "\n",
    "xx = np.arange(N_CLUSTERS) + 0.2\n",
    "ax[1].bar(xx,  yct, width=0.3, align='center', label='Ground Truth', color='brown')\n",
    "for i, val in enumerate(yct):\n",
    "    ax[1].text(i,   20+val, val)\n",
    "\n",
    "ax[1].set(xticks=range(N_CLUSTERS), xlim=[-1, N_CLUSTERS])    \n",
    "ax[1].set_xlabel('Cluster')\n",
    "ax[1].set_ylabel('#images')\n",
    "mm = np.bincount(Y).max()\n",
    "ax[1].set_ylim([None,mm+(20*mm)/100])\n",
    "ax[1].legend(loc='lower right')\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57e58d1-94bd-4812-a93c-29b725e7073d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
